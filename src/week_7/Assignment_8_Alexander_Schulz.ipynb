{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-24T20:54:24.928936Z",
     "end_time": "2023-05-24T20:54:24.951924Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 column names: ['Date', 'Daily minimum temperatures']\n",
      "Dataset 2 column names: ['Month', 'Monthly beer production']\n",
      "Dataset 1 shape: (3650,)\n",
      "Dataset 2 shape: (476,)\n",
      "[20.7 17.9 18.8 ... 13.5 15.7 13. ]\n"
     ]
    }
   ],
   "source": [
    "# Import the datasets\n",
    "csv1 = pd.read_csv('kaggle/daily-minimum-temperatures-in-me.csv')\n",
    "csv2 = pd.read_csv('kaggle/monthly-beer-production-in-austr.csv')\n",
    "\n",
    "# Convert the datasets to numpy arrays\n",
    "colnames_csv1 = list(csv1.columns)\n",
    "colnames_csv2 = list(csv2.columns)\n",
    "\n",
    "print(f\"Dataset 1 column names: {colnames_csv1}\")\n",
    "print(f\"Dataset 2 column names: {colnames_csv2}\")\n",
    "\n",
    "dataset1 = csv1[\"Daily minimum temperatures\"].astype(float).values\n",
    "dataset2 = csv2[\"Monthly beer production\"].astype(float).values\n",
    "\n",
    "print(f\"Dataset 1 shape: {dataset1.shape}\")\n",
    "print(f\"Dataset 2 shape: {dataset2.shape}\")\n",
    "print(dataset1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-24T20:54:27.681951Z",
     "end_time": "2023-05-24T20:54:27.708088Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1:\n",
      "Window Size: 4\n",
      "23/23 [==============================] - 0s 1ms/step\n",
      "23/23 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (730,) (726,) ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[38], line 53\u001B[0m\n\u001B[0;32m     50\u001B[0m lstm_model\u001B[38;5;241m.\u001B[39mfit(np\u001B[38;5;241m.\u001B[39marray(train_X)\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, window_size), np\u001B[38;5;241m.\u001B[39marray(train_y),\n\u001B[0;32m     51\u001B[0m                epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     52\u001B[0m lstm_predictions \u001B[38;5;241m=\u001B[39m lstm_model\u001B[38;5;241m.\u001B[39mpredict(np\u001B[38;5;241m.\u001B[39marray(test_X)\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, window_size))\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[1;32m---> 53\u001B[0m lstm_mse \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlstm_predictions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;66;03m# LSTM (batch mode)\u001B[39;00m\n\u001B[0;32m     56\u001B[0m lstm_batch_model \u001B[38;5;241m=\u001B[39m Sequential()\n",
      "Cell \u001B[1;32mIn[38], line 6\u001B[0m, in \u001B[0;36mevaluate_model\u001B[1;34m(true, pred)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluate_model\u001B[39m(true, pred):\n\u001B[1;32m----> 6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (np\u001B[38;5;241m.\u001B[39msquare(\u001B[43mtrue\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mpred\u001B[49m))\u001B[38;5;241m.\u001B[39mmean(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: operands could not be broadcast together with shapes (730,) (726,) "
     ]
    }
   ],
   "source": [
    "# Set up sliding window sizes\n",
    "window_sizes = range(4, 13)  # Range from 4 to 12\n",
    "\n",
    "# Define evaluation metric\n",
    "def evaluate_model(true, pred):\n",
    "    return (np.square(true - pred)).mean(axis=0)\n",
    "\n",
    "# Loop over the datasets\n",
    "for i, dataset in enumerate([dataset1, dataset2]):\n",
    "    print(f\"Dataset {i+1}:\")\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    train_size = int(len(dataset) * 0.8)  # 80% for training\n",
    "    train_data, test_data = dataset[:train_size], dataset[train_size:]\n",
    "\n",
    "    # Loop over the window sizes\n",
    "    for window_size in window_sizes:\n",
    "        print(f\"Window Size: {window_size}\")\n",
    "\n",
    "        # Prepare the data for sliding windows\n",
    "        train_X, train_y = [], []\n",
    "        test_X, test_y = [], []\n",
    "        for j in range(len(train_data) - window_size):\n",
    "            train_X.append(train_data[j:j+window_size])\n",
    "            train_y.append(train_data[j+window_size])\n",
    "        for j in range(len(test_data) - window_size):\n",
    "            test_X.append(test_data[j:j+window_size])\n",
    "            test_y.append(test_data[j+window_size])\n",
    "\n",
    "        # ARIMA\n",
    "        arima_model = ARIMA(train_data, order=(1, 0, 0))  # Example order, modify as needed\n",
    "        arima_model_fit = arima_model.fit()\n",
    "        arima_predictions = arima_model_fit.forecast(steps=len(test_data))[0]\n",
    "        arima_mse = evaluate_model(test_y, arima_predictions)\n",
    "\n",
    "        # NN\n",
    "        nn_model = Sequential()\n",
    "        nn_model.add(Dense(10, input_dim=window_size, activation='relu'))\n",
    "        nn_model.add(Dense(1))\n",
    "        nn_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        nn_model.fit(np.array(train_X), np.array(train_y), epochs=50, batch_size=16, verbose=0)\n",
    "        nn_predictions = nn_model.predict(np.array(test_X)).flatten()\n",
    "        nn_mse = evaluate_model(test_y, nn_predictions)\n",
    "\n",
    "        # LSTM (normal mode)\n",
    "        lstm_model = Sequential()\n",
    "        lstm_model.add(LSTM(10, input_shape=(1, window_size)))\n",
    "        lstm_model.add(Dense(1))\n",
    "        lstm_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        lstm_model.fit(np.array(train_X).reshape(-1, 1, window_size), np.array(train_y),\n",
    "                       epochs=50, batch_size=16, verbose=0)\n",
    "        lstm_predictions = lstm_model.predict(np.array(test_data).reshape(-1, 1, window_size)).flatten()\n",
    "        lstm_mse = evaluate_model(test_data, lstm_predictions)\n",
    "\n",
    "        # LSTM (batch mode)\n",
    "        lstm_batch_model = Sequential()\n",
    "        lstm_batch_model.add(LSTM(10, batch_input_shape=(16, 1, window_size), stateful=True))\n",
    "        lstm_batch_model.add(Dense(1))\n",
    "        lstm_batch_model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001))\n",
    "        for epoch in range(50):\n",
    "            lstm_batch_model.fit(np.array(train_X).reshape(-1, 1, window_size), np.array(train_y),\n",
    "                                 epochs=1, batch_size=16, verbose=0, shuffle=False)\n",
    "            lstm_batch_model.reset_states()\n",
    "        lstm_batch_predictions = lstm_batch_model.predict(np.array(test_data).reshape(-1, 1, window_size), batch_size=16).flatten()\n",
    "        lstm_batch_mse = evaluate_model(test_data, lstm_batch_predictions)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"ARIMA MSE: {arima_mse}\")\n",
    "        print(f\"NN MSE: {nn_mse}\")\n",
    "        print(f\"LSTM (Normal) MSE: {lstm_mse}\")\n",
    "        print(f\"LSTM (Batch) MSE: {lstm_batch_mse}\")\n",
    "        print(\"---\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-24T21:29:59.609145Z",
     "end_time": "2023-05-24T21:30:11.507375Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
